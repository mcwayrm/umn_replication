---
title: "Consolidated Argument"
subtitle: "Null Result Penalty Replication"
author: 
  - "Ryan McWay"
  - "Emily Kurtz"
date: "2025-03-18"
format: pdf
---

```{r}
#| echo: false
#| label: preamble
#| error: false
#| warning: false

#########################
# Preamble
#########################
# Clear environment and set encoding
rm(list=ls())
# Necessary Dependencies
library(rio) # Import any data type
library(ggplot2) # plots
library(data.table) # Processing data frames
library(dplyr) # Data editing
library(stargazer) # Output tables
library(plm) # Fixed effects
library(sandwich) # Standard errors
library(quantreg) # Quantile regressions

#########################s
# Directories 
#########################
path_data = "D:/umn_replication/original-data/data/main_study_cleaned.dta" # original data 
path_out = "D:/umn_replication/outputs/" # output folder

# Load data 
df <- setDT(import(path_data))
```

# Direct Replication

The direct replication was sucessful. But the paper seems almost too good to be true. 
The point of the paper is the null results are penalized for publication. 
Yet all the results, even the appendix results, have huge statistically significant effects. 

This is strange get the sample. They survey economists and ask them if they would publish a paper. 
This is measured on a sliding scale of 0 to 100. They provide each person with four of five vignettes. 
The authors take the vigenttes from real studies that are statistically significant and published. 
They keep the standard errors the same, but randomize if they shift the coefficient left in the distribution such that the effect is now statistically insignficant. 

The get a sample of 480 respondents who complete four vigenettes for 1920 observations. 
On top of that they cross-randomize 6 other attributes of the vigenettes. 
Aspects such as gender, prestige, etc. could effect if the finding is publishable beyond statistical significance. 
This produces 48 treatment assignments using a factorial design. 
In practice, the authors have 40 observations per treatment assignment to identify off of -- 10 respondents. 
Despite these small clusters, the standard errors are tiny. 
This makes us suspiciuos. 

![Factorial Design](../outputs/factorial-design.png)


As part of the reproduction, we identify Table 3 and Figure 2 has presenting the main effects. 
Table 3 is of primary interest as it estimates the null result effect on the primary outcome of interest and the secondary outcomes. 
Figure 2 estimates the interaction effect of the null effect with the cross-randomized characteristics of the vigenettes. 
Below we represent a reproduction of the main estimate -- Column 1 of Table 3. 
In addition, we are able to reproduce all the results from the replication packet provided by the authors using the original Stata code. 

```{r}
#| label: reproduced
#| error: false
#| warning: false

# Column 1
col1 <- plm(publish ~ low + exlow + exhigh + field + phd + unilow + pval, 
                data = df,
                index = c("id", "vignette"), 
                model = "within")
col1_se <- sqrt(diag(vcovHC(col1, type = "HC1", cluser = "id")))
    # TODO: Issue with adding clustering 
df_control <- subset(df, df$low == 0)
col1_mean <- round(mean(df_control$publish), 3) # Subset for control
# Present 
stargazer(col1,
        type = "text", 
        keep = c(1),
        covariate.labels = c("Null result treatment"),
        se = list(col1_se), 
        keep.stat = c("n", "adj.rsq"),
        model.numbers = TRUE,
        digits = 3, 
        add.lines = list(c("Mean Dep. Var.", col1_mean)
        )) 
```

We examine this in a couple of ways in particular. 

1. Variation in the dependent variable
2. Sample composition 
3. Quantile Regressions 
4. Propensity score matching 

The motivation for these robustness checks are to stress test the results in examining if there is potential data manipulation that ensures statistical significance. 
Our current results suggest that the data is unlikely to have been generated from real world data. 
The recommendation of this replication is that Chopra et al. (2023) should be replicated using new data with an independent team of researchers. 


# Variation in the Publishability 

The first thing that we note is the distribution of the primary outcome of interest -- publishability. 

The first thing that is strange is that the outcome measure appears to be uniformly distributed. That is a bit odd. 
Without binning, we also see that there is some grouping around divisors of 5 along the sliding scale used by respondents. 

```{r}
#| label: hist-DV
#| error: false
#| warning: false

# Suspiciously uniform
hist(df$publish, breaks = 10)
# Bunching around specific values
hist(df$publish, breaks = 100) # Lots of grouping on individual values 
```

We notice something strange when we examine the distribution of the outcome measure when highlighting treatment assignment. 
Notably, the control and treatment distributions look like mirrors of one another. 

```{r}
#| label: DV-by-treat
#| error: false
#| warning: false


# Publish by treatment
ggplot(df, aes(x = publish, fill = factor(low))) +
    geom_density(alpha = 0.5) +
    labs(title = "Density Plot of Publishability by Treatment",
         x = "Likilihood to Publish",
         y = "Density",
         fill = "Low") +
    theme_minimal()
```

We suspect that treatment and control are the same distribution but systemtric about the middle of the range (50).
In context, this is meaninful as 50 can be interperted as the threshold between publishing and not publishing the article. 
When we flip the control group distribution by the forumla $[publish|t_i = 0] = 100 - publish$ we find that treatment and control have the same distribution. 
This suggests that the data could have been generated from a random distribution rather than real data. 
In particular, this appears to be a Beta distribution. Using the following formula for the probability distribution function, you could reproduce the underlying data, split the sample in half, and flip the 'control' group about the range to create a reflection.
With this reflection, we could produce the results from the Chopra et al. paper without collecting any data. 

> The PDF for the beta distribution, for $0 \le x \le 1$, uses the shape parameters $\alpha, \beta > 0$ to create a power function of some variable $x$. The denominator is normalization to ensure total probability of 1.

$$
f(x; \alpha, \beta) = \frac{x^{\alpha -1} (1-x)^{\beta -1}}{\int_0^1 u^{\alpha-1} (1-u)^{\beta -1} du}
$$


```{r}
#| label: hist-overlay
#| error: false
#| warning: false


# Histogram overlaying control onto treatment
df2 <- data.table::copy(df)
df2[, publish := ifelse(low == 1, publish, 100 - publish)]

# Regular historgram by treatment
ggplot(df, aes(publish, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge', binwidth = 1)
# Histogram after flip the scale (e.g., are the symettric about the average (50))
ggplot(df2, aes(publish, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge', binwidth = 1)
    
    # This looks awfully symettric...
```

One thing that the authors could have done to make the data appear more 'realistic' is to 'jitter' the data in the distrbution and apply a heurisitic for how participants would select values. 
Suppose that we expect people to tend to select items that are multiple of 5s or 10s. 
Then I could just create this Beta distribution as a discrete function with intervals of fives. 
For the formula above, instead of $\int$ you could replace it with $\sum_i^n f(5i)$ to make this discrete distribution.
Because this would be too neat, the authors may add some noise. 
Specifically, values that are not multiples of 5, as well as adding values near multiples of 5 to show human errors. 


We account for this in our descriptive of the distribution by recoding values near divisors of 5 to the nearest divisor. 
As a bandwidth, we recode values that are 1 value away.
For example, if you have a uniform distrbution from 5 to 10 you would expect the observations: $5,6,7,8,9,10$. 
Using our bandwidth to recode we will now have the observations $5,5,7,8,10,10$.
In a uniform distribution, that means that rather than a 2/6 chance of selection for divisors of 5 there is now a 4/6 chance of divisors of 5. 
The increased likelihood should apply similiarly to the Beta distribution. 

```{r}
#| label: hist-recode
#| error: false
#| warning: false


# Recode values within bandwidth 
df = df[, publish := fifelse(publish %% 5 == 0, publish,
                fifelse(publish %% 5 == 1, publish - 1,
                fifelse(publish %% 5 == 4, publish + 1, publish)))]

# Same for overlay data set
df2 = df2[, publish := fifelse(publish %% 5 == 0, publish,
                fifelse(publish %% 5 == 1, publish - 1,
                fifelse(publish %% 5 == 4, publish + 1, publish)))]

# Replot 
ggplot(df, aes(publish, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge', binwidth = 1) # As presented
ggplot(df2, aes(publish, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge', binwidth = 1) # With overlay
```


Content to add here Emily: 
- Details on distribution from other slider bars. In particular if we could get some that are from other studies predicting things on a slider from 0 to 100. 
- The empircal tests: kolmogorov Smirnov test 


# Variation in Secondary Outcomes 

In brief, we do not find signs of potential manipulation for the the secondary outcomes like we do for the primary outcome for publication. We examine them through similar replication of Table 3 results and exploring histograms for the secondary outcomes. These histograms are presenting the opposite relationship that was found for the primary outcome. There is considerable overlap in the original distribution -- a more reasonable generated data set. Note that z-scores are what are estimated in the paper. So I present those histograms and then show esimates of Table 3 for the secondary outcomes before and after the z-score moditification. 

- First order quality 

```{r}
#| label: 2nd-outcome-hist-1
#| error: false
#| warning: false

# Original
ggplot(df, aes(qualityfob, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge', binwidth = 1)
# Fliped 
df2[, qualityfob := ifelse(low == 1, qualityfob, 100 - qualityfob)]
ggplot(df2, aes(qualityfob, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge', binwidth = 1)
# Z-score
ggplot(df, aes(z_qualityfob, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge')
```

- Second order quality 

```{r}
#| label: 2nd-outcome-hist-2
#| error: false
#| warning: false

# Original
ggplot(df, aes(qualitysob, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge', binwidth = 1)
# Fliped 
df2[, qualitysob := ifelse(low == 1, qualitysob, 100 - qualitysob)]
ggplot(df2, aes(qualitysob, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge', binwidth = 1)
# Z-score
ggplot(df, aes(z_qualitysob, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge')
```

- First order importance

```{r}
#| label: 2nd-outcome-hist-3
#| error: false
#| warning: false

# Original
ggplot(df, aes(importancefob, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge', binwidth = 1)
# Fliped 
df2[, importancefob := ifelse(low == 1, importancefob, 100 - importancefob)]
ggplot(df2, aes(importancefob, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge', binwidth = 1)
# Z-score
ggplot(df, aes(z_importancefob, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge')
```

- Second order importance

```{r}
#| label: 2nd-outcome-hist-4
#| error: false
#| warning: false

# Original
ggplot(df, aes(importancesob, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge', binwidth = 1)
# Fliped 
df2[, importancesob := ifelse(low == 1, importancesob, 100 - importancesob)]
ggplot(df2, aes(importancesob, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge', binwidth = 1)
# Z-score
ggplot(df, aes(z_importancesob, fill = as.factor(low))) + 
    geom_histogram(alpha = 0.5, position = 'dodge')
```


This is the replication of Table 3 with the original z-score values. The values match very closely. But note that these are measured in standard deviations. A 0.3 or 0.4 standard deviation change is a massive effect size. 

```{r}
#| label: table3-2nd-outcomes
#| error: false
#| warning: false

# Column 2
col2 <- plm(z_qualityfob ~ low + exlow + exhigh + field + phd + unilow + pval, 
                data = df,
                index = c("id", "vignette"), 
                model = "within")
col2_se <- sqrt(diag(vcovHC(col2, type = "HC1", cluser = "id")))
    # TODO: Issue with adding clustering 
# Column 3 
col3 <- plm(z_qualitysob ~ low + exlow + exhigh + field + phd + unilow + pval, 
                data = df,
                index = c("id", "vignette"), 
                model = "within")
col3_se <- sqrt(diag(vcovHC(col3, type = "HC1", cluser = "id")))
# Column 4 
col4 <- plm(z_importancefob ~ low + exlow + exhigh + field + phd + unilow + pval, 
                data = df,
                index = c("id", "vignette"), 
                model = "within")
col4_se <- sqrt(diag(vcovHC(col4, type = "HC1", cluser = "id")))
# Column 5 
col5 <- plm(z_importancesob ~ low + exlow + exhigh + field + phd + unilow + pval, 
                data = df,
                index = c("id", "vignette"), 
                model = "within")
col5_se <- sqrt(diag(vcovHC(col5, type = "HC1", cluser = "id")))

# Means
df_control <- subset(df, df$low == 0) # Subset for control
col2_mean <- round(mean(df_control$z_qualityfob, na.rm = TRUE), 3) 
col3_mean <- round(mean(df_control$z_qualitysob, na.rm = TRUE), 3) 
col4_mean <- round(mean(df_control$z_importancefob, na.rm = TRUE), 3) 
col5_mean <- round(mean(df_control$z_importancesob, na.rm = TRUE), 3) 

# Present 
stargazer(col2, col3, col4, col5,
        type = "text", 
        keep = c(1),
        covariate.labels = c("Null result treatment"),
        se = list(col2_se, col3_se, col4_se, col5_se), 
        keep.stat = c("n", "adj.rsq"),
        model.numbers = TRUE,
        digits = 3, 
        add.lines = list(c("Mean Dep. Var.", col2_mean, col3_mean, col4_mean, col5_mean)
        )) 
```


Again, I replicate Table 3 but now with the original percentage point distribution (same units as Column 1 for primary outcome of interest). Again, the effect sizes are very statistically significant and large. But relative to the control group's dependent variable means, these effects are only shifting towards 50/50 decisions on measures of importance or quality for the paper. This is not flipping the decision as we see for the measure of publishability.

```{r}
#| label: table3-2nd-outcomes-noZ
#| error: false
#| warning: false

# Column 2
col2 <- plm(qualityfob ~ low + exlow + exhigh + field + phd + unilow + pval, 
                data = df,
                index = c("id", "vignette"), 
                model = "within")
col2_se <- sqrt(diag(vcovHC(col2, type = "HC1", cluser = "id")))
    # TODO: Issue with adding clustering 
# Column 3 
col3 <- plm(qualitysob ~ low + exlow + exhigh + field + phd + unilow + pval, 
                data = df,
                index = c("id", "vignette"), 
                model = "within")
col3_se <- sqrt(diag(vcovHC(col3, type = "HC1", cluser = "id")))
# Column 4 
col4 <- plm(importancefob ~ low + exlow + exhigh + field + phd + unilow + pval, 
                data = df,
                index = c("id", "vignette"), 
                model = "within")
col4_se <- sqrt(diag(vcovHC(col4, type = "HC1", cluser = "id")))
# Column 5 
col5 <- plm(importancesob ~ low + exlow + exhigh + field + phd + unilow + pval, 
                data = df,
                index = c("id", "vignette"), 
                model = "within")
col5_se <- sqrt(diag(vcovHC(col5, type = "HC1", cluser = "id")))

# Means
df_control <- subset(df, df$low == 0) # Subset for control
col2_mean <- round(mean(df_control$qualityfob, na.rm = TRUE), 3) 
col3_mean <- round(mean(df_control$qualitysob, na.rm = TRUE), 3) 
col4_mean <- round(mean(df_control$importancefob, na.rm = TRUE), 3) 
col5_mean <- round(mean(df_control$importancesob, na.rm = TRUE), 3) 

# Present 
stargazer(col2, col3, col4, col5,
        type = "text", 
        keep = c(1),
        covariate.labels = c("Null result treatment"),
        se = list(col2_se, col3_se, col4_se, col5_se), 
        keep.stat = c("n", "adj.rsq"),
        model.numbers = TRUE,
        digits = 3, 
        add.lines = list(c("Mean Dep. Var.", col2_mean, col3_mean, col4_mean, col5_mean)
        )) 
```


# Salience of Treatment 

The treatment for the null result treatment and the cross-randomized vigenette characteristics are presented through paragraphs the reviewer reads. These are short paragraphs. But some respondents take very little time or a very long time to respond to each vigenette. Therefore, we can use the time duration for each vigenette as a measure of salience that the respondent is (1) paying attention and (2) absorbing the treatment. For example, we show what the vigenettes look like to the participants (from the online appendix). 

![Vigenette Example](../outputs/vigenette-example.png)

First, I explore for outliers. I start by looking at the tails of the distribution. I note that there is a very long right tail in time. This suggests that some people open the survey, leave it in the background, and then come back to it. This is times at the vigenette level, not overall. So this ideally is not a measure of people not closing out of the survey. I examing times over 10,000 seconds (166 minutes). This is 5% of the sample. That is reasonable as 1 in 20 folks are getting distracted. But if I lower this to 2000 seconds (33 minutes), the proportion is 31% of the sample. This suggests that a large portion of the sample is take a very considerably long time to make a decision on the short paragraph above. This is not necessarily bad, it is just a bit suprising. On the other hand, I examine folks for whom they may not be examining the information closely. These are folks who read the paragraph perhaps too quickly, and by consequence are not recieving a salient treatment. About 20% of respondents are completing the vigenette section in under a minute. With seasoned eyes, perhaps that is reasonable. But it does suggest the respondents are just glossing over the information rather than reading carefully. If I restrict this to only 30 seconds, only 2.5% of the sample respondents are replying very quickly. What I find suspicious is how exact these values are for the lower measures. 


If we explore the distribution (cutting off the long right tail at 2000 seconds -- removing 30% of the sample), we can see the influence of treatment as a salient effect and order effects of the vigenettes as the respodent learns (gets quicker) or gets bored (gets slower). The average is 177 seconds (3 minutes) to read the vigenette and respond. The treatment groups have considerable overlap. And the order effects show there is some learning to become quicker over time. 

```{r}
#| label: time-on-vigenettes
#| error: false
#| warning: false

# Number of Observations in each tail of the distribution 
time <- 10000 # 166.67 minutes
num_above_threshold <- sum(df$pagetime > time) # n above threshold
total_observations <- nrow(df) # n
percent_above_threshold <- (num_above_threshold / total_observations) * 100 # percent
percent_above_threshold # 5%
print(paste0("Precentage of Vigenette Times Above ", time, " Seconds: ", format(percent_above_threshold, digits = 3)))

time <- 2000 # 33 minutes
num_above_threshold <- sum(df$pagetime > time) # n above threshold
total_observations <- nrow(df) # n
percent_above_threshold <- (num_above_threshold / total_observations) * 100 # percent
percent_above_threshold # 31%
print(paste0("Precentage of Vigenette Times Above ", time, " Seconds: ", format(percent_above_threshold, digits = 3)))

time <- 60 # 1 minute
num_above_threshold <- sum(df$pagetime < time) # n above threshold
total_observations <- nrow(df) # n
percent_above_threshold <- (num_above_threshold / total_observations) * 100 # percent
percent_above_threshold # 20%
print(paste0("Precentage of Vigenette Times Below ", time, " Seconds: ", format(percent_above_threshold, digits = 3)))

time <- 30 # 1/2 minute
num_above_threshold <- sum(df$pagetime < time) # n above threshold
total_observations <- nrow(df) # n
percent_above_threshold <- (num_above_threshold / total_observations) * 100 # percent
percent_above_threshold # 2.5%
print(paste0("Precentage of Vigenette Times Below ", time, " Seconds: ", format(percent_above_threshold, digits = 3)))

# Overall Distribution
ggplot(subset(df, df$pagetime < 2000), aes(x = pagetime)) +
    geom_density(alpha = 0.5) +
    labs(title = "Density Plot of Time Spent on Vigenettes",
         x = "Seconds",
         y = "Density",
         fill = "Low") +
    theme_minimal()

# By Treatment Status
ggplot(subset(df, df$pagetime < 2000), aes(x = pagetime, fill = factor(low))) +
    geom_density(alpha = 0.5) +
    labs(title = "Density Plot of Time Spent on Vigenettes",
         x = "Seconds",
         y = "Density",
         fill = "Low") +
    theme_minimal()

# Order Effects
ggplot(subset(df, df$pagetime < 2000), aes(x = pagetime, fill = factor(order))) +
    geom_density(alpha = 0.5) +
    labs(title = "Density Plot of Time Spent on Vigenettes",
         x = "Seconds",
         y = "Density",
         fill = "Low") +
    theme_minimal()
```

- Control and interaction effect of vigenette times for Table 3 

- Create an abitrary cut off for short and long. Trim the data and re-estimate.

# Sample Composition 

- Ryan
- Two things: include the two sample selections they edit and...
- Remove observations that may not have salience of treatment (short and long duration or vigenette observations as well as 'finished == 0' observations)
- Re-estimate Table 3 effects

# Quantile Regressions 

- Ryan
- Derek might need to remind me of the motivation here...

```{r}
#| label: quant-regs 
#| error: false
#| warning: false


# Quantile Regression: 
    # Tau is quantile: Repeat for 0.1 to 0.9.
taus <- seq(from = .1, to = .9, by = 0.1) # Range of quantiles
quant_all  <- rq(publish ~ low + exlow + exhigh + field + phd + unilow + pval + id + vignette, 
                tau = taus, 
                data = df)

print(quant_all$coef)
print(quant_all$coef[2,])
# NOTE: So there is variation over the quantiles... good sign

# q01_se <- sqrt(diag(vcovHC(q_05, type = "HC1", cluser = "id")))
#     # TODO: Issue with quantile regression with FE 
#     # TODO: Do for other values 

# # Present 
# stargazer(col1,
#         type = "text", 
#         keep = c(1),
#         covariate.labels = c("Null result treatment"),
#         se = list(col1_se), 
#         keep.stat = c("n", "adj.rsq"),
#         model.numbers = TRUE,
#         digits = 3, 
#         add.lines = list(c("Mean Dep. Var.", col1_mean)
#         )) 
# # Summarize the results
# summary(quantile_reg)
```


```{r}
#| label: quant_plots
#| error: false
#| warning: false

# TODO: Need to recover the SE to create CI for the plots. 


# Plot the quantile regressions 
# plot_models(ols, quant_reg_med, quant_reg_first, quant_reg_last,
#            show.values = TRUE,
#            m.labels = c("OLS", "Median", "10th percentile",
#                        "95th percentile",
#                        legend.title = "Model")
#            )
```


# Propensity Score Matching 

- Ryan/Derek...
- Create Propensity scores with logit 
- Do matching 
- Estimated effect on matched pairs

# Data Patterns

- Plotting responds by their apperance in the data

# P-curve and t-curve

- Compare to the Cochrane or economics distribution of z-scores or t-scores 
- https://www.aeaweb.org/articles?id=10.1257/aer.20190687
- Note that the study is underpowered -- should expect to see null effects 

- Do this for the presented results

- Do this for the new estimated effects

# Median is Static 

- No matter the sample difference in each randomized group, it is almost always 50. 